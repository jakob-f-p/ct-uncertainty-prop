import csv
import json
import logging.config
import logging.handlers
import numpy as np
import radiomics
import SimpleITK as sitk
import threading
import warnings

from collections import OrderedDict
from datetime import datetime
from feature_extraction_cpp import (extraction_params_file, FeatureData, feature_directory,
                                    VtkImageMaskPair, VtkImageData, VtkType)
from functools import partial
from multiprocessing import cpu_count, Manager, Pool
from multiprocessing.pool import AsyncResult
from pathlib import Path
from radiomics import featureextractor
from radiomics.scripts import segment
from typing import cast, Dict, Callable, List, Tuple

warnings.filterwarnings(action="ignore", category=RuntimeWarning)

logger = logging.getLogger(__file__)

FeatureExtractionResult = List[OrderedDict[str, float]]


class SitkImageMaskPair:
    def __init__(self, image: sitk.Image, mask: sitk.Mask):
        self.Image = image
        self.Mask = mask


def do_extraction(case, extractor, logging_config) -> OrderedDict:
    try:
        return segment.extractSegment_parallel(case,
                                               extractor=extractor,
                                               logging_config=logging_config)
    except ValueError:
        return OrderedDict()


def vtk_to_sitk_image(vtk_image: VtkImageData) -> sitk.Image:
    dims = list(vtk_image.get_dimensions())
    origin = vtk_image.get_origin()
    spacing = vtk_image.get_spacing()

    vtk_data_type: VtkType = vtk_image.get_data_type()
    vtk_to_np_type_dict = {VtkType.Short: np.short, VtkType.Float: np.float32}
    np_data_type = vtk_to_np_type_dict[vtk_data_type]

    np_data = np.frombuffer(vtk_image, dtype=np_data_type)

    dims.reverse()
    np_data.shape = tuple(dims)

    sitk_image = sitk.GetImageFromArray(np_data)
    sitk_image.SetSpacing(spacing)
    sitk_image.SetOrigin(origin)

    return sitk_image


def vtk_pairs_to_sitk_pairs(vtk_image_mask_pairs: List[VtkImageMaskPair]) -> List[SitkImageMaskPair]:
    sitk_pairs: List[SitkImageMaskPair] = []

    for vtk_image_mask_pair in vtk_image_mask_pairs:
        vtk_image: VtkImageData = vtk_image_mask_pair.image
        vtk_mask: VtkImageData = vtk_image_mask_pair.mask

        sitk_pair = SitkImageMaskPair(vtk_to_sitk_image(vtk_image), vtk_to_sitk_image(vtk_mask))
        sitk_pairs.append(sitk_pair)

    return sitk_pairs


class FeatureExtraction:
    params_file: Path
    vtk_image_mask_pairs: List[VtkImageMaskPair]
    sitk_image_mask_pairs: List[SitkImageMaskPair]
    number_of_images: int
    is_multiprocessing: bool
    out_dir: Path
    log_file: Path
    logging_config: Dict
    logging_queue_listener: logging.handlers.QueueListener
    progress_callback: Callable[[float], None]

    debug: bool = False

    def __init__(self, vtk_image_mask_pairs: List[VtkImageMaskPair], progress_callback: Callable[[float], None]):
        self.params_file = extraction_params_file
        self.vtk_image_mask_pairs = vtk_image_mask_pairs
        self.sitk_image_mask_pairs = vtk_pairs_to_sitk_pairs(self.vtk_image_mask_pairs)
        self.number_of_images = len(vtk_image_mask_pairs)
        self.is_multiprocessing = self.number_of_images > 1
        self.out_dir = feature_directory
        self.log_file = self.out_dir / "log.txt"
        self.logging_config, self.logging_queue_listener = self.get_logging_config()
        self.progress_callback = progress_callback

    def run(self) -> FeatureExtractionResult:
        results = []
        try:
            logger.info("Starting PyRadiomics (version: %s)", radiomics.__version__)

            cases = self.get_cases()

            results = self.extract_features(cases)

            if self.debug:
                self.write_outputs(results)

            logger.info("Finished extraction successfully...")
        except (KeyboardInterrupt, SystemExit):
            logger.info("Cancelling Extraction")
        finally:
            if self.logging_queue_listener is not None:
                for item in self.logging_queue_listener.handlers:
                    if isinstance(item, logging.Handler):
                        handler = cast(logging.Handler, item)
                        handler.close()
                self.logging_queue_listener.stop()

        result_feature_dicts: FeatureExtractionResult = []
        for res_dict in results:
            res_feature_dict = OrderedDict(res_dict)
            for key in list(res_feature_dict):
                if not cast(str, key).startswith("original"):
                    del res_feature_dict[key]
            result_feature_dicts.append(cast(OrderedDict[str, float], res_feature_dict))

        for res_feature_dict in result_feature_dicts:
            for key, value in res_feature_dict.items():
                res_feature_dict[key] = np.float64(value)

        return result_feature_dicts

    def get_logging_config(self) -> (Dict, logging.handlers.QueueListener):
        queue_listener = None

        verbose_level = 30
        logger_level = 50

        self.log_file.unlink(missing_ok=True)

        logging_config = {
            "version": 1,
            "disable_existing_loggers": False,
            "formatters": {
                "default": {
                    "format": "[%(asctime)s] %(levelname)-.1s: %(name)s: %(message)s",
                    "datefmt": "%Y-%m-%d %H:%M:%S"
                }
            },
            "handlers": {
                "console": {
                    "class": "logging.StreamHandler",
                    "level": verbose_level,
                    "formatter": "default"
                }
            },
            "loggers": {
                "radiomics": {
                    "level": logger_level,
                    "handlers": ["console"]
                }
            }
        }

        # Set up logging to file
        if self.is_multiprocessing:
            q = Manager().Queue(-1)
            threading.current_thread().setName("Main")

            logging_config["formatters"]["default"]["format"] = ("[%(asctime)s] %(levelname)-.1s:"
                                                                 "(%(threadName)s) %(name)s: %(message)s")

            logging_config["handlers"]["logfile"] = {
                "class": "logging.handlers.QueueHandler",
                "queue": q,
                "level": logger_level,
                "formatter": "default"
            }

            file_handler = logging.FileHandler(filename=self.log_file, mode="a")
            file_handler.setFormatter(logging.Formatter(fmt=logging_config["formatters"]["default"].get("format"),
                                                        datefmt=logging_config["formatters"]["default"].get("datefmt")))

            queue_listener = logging.handlers.QueueListener(q, file_handler)
            queue_listener.start()
        else:
            logging_config["handlers"]["logfile"] = {
                "class": "logging.FileHandler",
                "filename": self.log_file,
                "mode": "a",
                "level": logger_level,
                "formatter": "default"
            }
        logging_config["loggers"]["radiomics"]["handlers"].append("logfile")

        logging.config.dictConfig(logging_config)

        return logging_config, queue_listener

    def get_cases(self) -> List[Tuple[int, Dict[str, str]]]:
        generator = []
        for i in range(0, self.number_of_images):
            pair = self.sitk_image_mask_pairs[i]
            generator.append((i + 1, {"Image": pair.Image, "Mask": pair.Mask}))

        return generator

    def extract_features(self, cases) -> List[OrderedDict]:
        extractor = featureextractor.RadiomicsFeatureExtractor(str(self.params_file))

        results = []
        number_of_workers = min(cpu_count() - 1, self.number_of_images)
        if self.is_multiprocessing and number_of_workers > 1:
            async_results: List[AsyncResult] = []
            with Pool(number_of_workers) as pool:
                for case in cases:
                    async_results.append(pool.apply_async(partial(segment.extractSegment_parallel,
                                                                  case,
                                                                  extractor=extractor,
                                                                  logging_config=self.logging_config)))

                done: bool = False
                last_successful = -1
                while not done:
                    running, successful, error = 0, 0, 0
                    for result in async_results:
                        try:
                            if result.successful():
                                successful += 1
                            else:
                                error += 1
                        except ValueError:
                            running += 1

                    # if error > 0:
                    #     raise ValueError

                    if successful + error > last_successful:
                        self.progress_callback(successful / self.number_of_images)
                        last_successful = successful + error

                    if successful + error == len(cases):
                        done = True

            results = [res.get() if res.successful() else OrderedDict() for res in async_results]
        else:
            for (i, case) in enumerate(cases):
                self.progress_callback(i / self.number_of_images)
                results.append(segment.extractSegment(*case, extractor=extractor))

        # set feature values of empty results to 0
        lengths = [len(result) for result in results]
        max_length = max(lengths)
        max_idx = lengths.index(max_length)
        max_element = results[max_idx]
        for result in results:
            if len(result) < max_length:
                for max_key in max_element:
                    if max_key not in result:
                        result[max_key] = max_element[max_key] if not max_key.startswith("original") else 0.0

        return results

    def write_outputs(self, results):
        logger.info("Processing results...")

        for result in results[1:]:
            if list(result.keys()) != list(results[0].keys()):
                raise ValueError("Results do not have the same keys")

        headers = list(results[0].keys())

        time_stamp = datetime.now()
        time_stamp_string = time_stamp.strftime("%Y-%m-%d-%H-%M-%S")
        csv_filepath: Path = self.out_dir / "results-debug-{}.csv".format(time_stamp_string)
        json_filepath: Path = self.out_dir / "results-debug-{}.json".format(time_stamp_string)

        for filepath in [csv_filepath, json_filepath]:
            open(str(filepath), "w").close()  # clear

        for case_idx, case in enumerate(results, start=1):
            with open(csv_filepath, "a") as csv_file:
                writer = csv.DictWriter(csv_file, headers, lineterminator="\n", extrasaction="ignore")
                if case_idx == 1:
                    writer.writeheader()
                writer.writerow(case)

        class NumpyEncoder(json.JSONEncoder):
            def default(self, obj):
                if isinstance(obj, np.ndarray):
                    return obj.tolist()

                if isinstance(obj, sitk.Image):
                    return id(obj)

                return json.JSONEncoder.default(self, obj)

        with open(json_filepath, "w+") as json_file:
            json.dump(results, json_file, cls=NumpyEncoder, indent=2)


def extract(image_mask_pairs: List[VtkImageMaskPair],
            progress_callback: Callable[[float], None] = lambda f: None) -> FeatureExtractionResult:

    extraction = FeatureExtraction(image_mask_pairs, progress_callback)

    result: FeatureExtractionResult = extraction.run()

    feature_names: List[str] = list(result[0].keys())
    feature_values: List[List[float]] = []
    for ordered_dict in result:
        feature_values.append(list(ordered_dict.values()))

    feature_data = FeatureData(feature_names, feature_values)

    return feature_data
